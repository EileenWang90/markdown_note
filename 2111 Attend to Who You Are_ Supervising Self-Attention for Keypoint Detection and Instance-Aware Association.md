# [论文阅读：姿态识别&Transformer] Attend to Who You Are: Supervising Self-Attention for Keypoint Detection and Instance-Aware Association

[TOC]

来自东南大学、旷视、清华大学

paper https://arxiv.org/pdf/2111.12892.pdf 

刚看的姿态识别相关论文，前两篇也是旷视同一团队的，[TokenPose](https://blog.csdn.net/cheerful090/article/details/122068852) 和 [SimDR](https://blog.csdn.net/cheerful090/article/details/122087398)，具体分析可见链接。

+ **Bottom-up**，使用Transformer实现了多人关键点识别、分组和人体实例分割。

+ Follow TransPose的姿态识别网络，比较**创新**的一点是**分组算法**，使用了transformer中的注意力矩阵作为相关性度量，提出了body-first和part-first的分组算法。

  （这儿多说一点，因为self-attention编码的就是一个点与所有其他点的相关性，那正好可以通过未匹配的点与当前各实例的平均距离来分组，还挺巧妙的。这时也就不需要多加向量场之类的信息了，如OpenPose中采用的部分亲和场PAF）

+ **监督的self-attention**, 使用实例掩码进行监督

+ 效果还没有超过CNN架构的Bottom-up姿态识别，而Transormer的二次复杂度也有待改进。

###  摘要：
本文提出了一种利用**Transformer**解决**关键点检测**和**实例关联**问题的新方法。对于自底向上(**Bottom up**)的多人姿态估计模型，需要<u>检测关键点并学习关键点之间的关联信息</u>。我们认为，Transformer可以完全解决这些问题。具体来说，<u>vision Transformer中的自注意度量任何一对位置之间的依赖关系，这可以为关键点分组提供关联信息</u>。然而，朴素注意模式仍然没有被主观控制，因此不能保证关键点总是注意到它们所属的实例。为了解决这一问题，我们提出了一种**监督多人关键点检测和实例关联的自我注意方法**。通过使用**实例掩码(instance mask)**来监督自注意，使其具有<u>实例感知性</u>，我们可以根据成对的注意分数将检测到的关键点分配给相应的实例，而无需使用预定义的偏移向量字段或像基于CNN的自底向上模型那样的嵌入。该方法的另一个优点是，可以<u>直接从监督注意矩阵中获得任意人数的实例分割结果</u>，从而简化了像素分配流程。通过对<u>COCO多人关键点检测</u>任务和<u>人实例分割任务</u>的实验，验证了该方法的有效性和简单性，为特定目的的自我注意行为控制提供了一种很有前景的方法。

###  1.Introduction
多人姿态估计方法通常可以分为两种:自顶向下(Top-down)和自底向上(Bottom-up)。与自顶向下的方法使用2个阶段：检测+单人姿态识别不同，自底向上的方法面临着更具有挑战性的问题。在输入图像的任何位置，可能会出现数量未知的人，他们具有任何比例、姿势或遮挡条件。**自底向上**的方法<u>需要首先检测所有的身体关节，然后将它们分组到人体实例中</u>。在**DeeperCut **(Insafutdinov et al.， 2016)、**OpenPose** (Cao et al.， 2017)、**Associative Embedding** (Newell et al.， 2017)、**PersonLab** (Papandreou et al.， 2018)、**PifPaf** (Kreiss et al.， 2019)和**CenterNet** (Zhou et al.， 2019)等典型系统中，**关键点检测**和**分组**通常被视为两个异构的学习目标。这需要模型学习关键点热图编码的位置信息和人类知识引导信号编码关联信息，如**部分假设(part hypotheses)、部分亲和场(part affinity fields)、关联嵌入(associative embeddings)或偏移向量场(offset vector fields)**。

在本文中，我们探讨是否我们可以利用**实例语义线索**，<u>来将检测到的关键点分组到单独的实例中</u>。我们的主要直觉是，<u>当模型预测特定关键点的位置时，它可能知道该关键点所属的人类实例区域，这意味着模型将相关关节隐含地关联在一起</u>。例如，当一个肘关节被识别时，模型可能会在邻近的手腕或肩膀上识别出它的强空间依赖性，但在其他人的关节上识别出弱空间依赖性。因此,<u>如果我们可以在模型中读出这些学习和编码的信息，检测到的关键点可以被正确地分组到实例中</u>，而不需要人类预定义的联想信号的帮助。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAf1_20211222194810.png" style="zoom:80%;" />

图1:我们选择了两个例子来展示**朴素自我注意模式**和**监督自我注意模式**之间的区别。原始自注意的关联引用来自Transformer中的所有注意层的平均值。监督自我注意的关联参考直接取自第四监督注意层。

我们认为，**基于自我注意的Transformer满足这一要求**，因为它<u>可以在任何一对位置之间提供特定于图像的成对相似性，而不受距离限制，并且产生的注意模式显示了与对象相关的语义</u>。因此，我们尝试利用自我注意机制进行多人姿态估计。但是，我们没有采用以单人区域作为输入的自顶向下策略，而是**向Transformer提供包含多人的高分辨率输入图像，并期望它输出编码多人关键点位置的热图**。初步结果表明，

1）<u>Transformer输出的热图也能准确响应多个候选位置的多人关键点</u>;

2）<u>检测到的关键点位置之间的注意得分在同一个体内较高，而在不同个体间较低</u>。

基于这些发现，我们引入了一种**基于注意力的解析算法来将检测到的关键点分组到不同的人实例中**。

不幸的是，naive的自我注意力并不总是表现出令人满意的特性。<u>在许多情况下，一个被检测到的关键点也可能与那些属于不同的人实例的关键点具有相对较高的注意力得分</u>。这肯定会导致错误的联想和令人难以置信的人类姿势。为了解决这个问题，我们提出了一种新的方法，**利用一个损失函数，通过每个人实例的掩码来显式地监督每个人实例的注意区域**。结果表明，该方法能在不影响Transformer标准正向传播的情况下，达到预期的实例判别特性。这些特征保证了基于注意力的分组算法的有效性和准确性。COCO关键点检测挑战的结果表明，与高度优化的自底向上位姿估计系统相比，我们的模型经过有限的改进，可以获得可与之媲美的性能(OpenPose; AE; PersonLab)。同时，我们也可以通过**对相应的注意区域进行采样**，**轻松获得人实例掩码**，从而<u>避免了额外的像素分配或分组算法。</u>

#### Contributions  

**Using self-attention to unify keypoint detection, grouping and human mask prediction**。

我们**使用Transformer以统一的方式解决具有挑战性的多人关键点检测、分组和人体掩码预测**。我们发现，自注意力显示了实例相关的语义，可以作为自底向上的关联信息。我们进一步使用**实例掩码**<u>来监督自我注意</u>。它确保根据注意力分数将每个关键点分配给正确的人工实例，从而也很容易获得实例掩码。

**Supervising self-attention “for your need”.** 

使用Transformer模型的一个常见实践是**使用task-specific的信号来监督基于Transformer的模型**的最终输出，例如**类标签、对象框坐标、关键点位置或语义掩码**。在这种方法中，一个**关键创新**是**使用某种类型的约束条件来控制自我注意的行为**。结果表明，在不破坏Transformer标准正向的前提下，自注意能够实现对多人姿态估计和掩模预测的实例感知特性。这表明，<u>使用适当的引导信号可以控制自我注意</u>，并帮助模型学习，<u>这也适用于其他视觉任务，如实例分割(Wang et al.， 2021)和目标检测(Carion et al.， 2020)。</u>



###  2. Method

####  2.1. Problem Setting
给定大小为3×H×W的RGB图像I，二维多人姿态估计的目标是估计出所有人的关键点位置: $\mathbb{S} =\{(x^k_i, y^k_i)|i = 1, 2，…，N; k = 1, 2，…，K\}$其中N是图中的人数，K是定义的关键点类型的数量。

我们遵循Bottom-up的战略。首先，模型检测出图像中每种类型关键点的所有候选位置: $C = C_1\cap C_2\cap ...\cap C_K其中C_K =\{(\hat{x}_i,\hat{y}_i)|i = 1,2,...,N_k\}$表示检测到Nk候选的第k种关键点集。其次，**启发式解码算法g根据关联信息A将所有候选节点分组为M个骨架**，确定每个关键点位置的唯一个人ID;我们制定这个过程:$g((\hat{x}_i,\hat{y}_i),\mathbb{C}, A)\to m\in \{1,2,…,M\}$。

接下来，我们给出了模型的结构，并<u>展示了如何使用自我注意作为关联信息A</u>，分析了使用朴素自我注意作为分组参考时存在的问题。我们建议**通过实例掩码来监督关键点分组的自我注意**。我们从**主体优先(body-first)**和**部分优先(part-first)**的观点提出了两种**分组算法**。最后，我们将描述<u>如何获得人员实例掩码(person instance masks)，以及如何使用获得的掩码来优化结果</u>。

#### 2.2. 网络架构和naive self-attention

**网络架构** 我们使用了一个简单的架构组合，包括<u>ResNet 和Transformer编码器</u>，参照**TransPose**的设计。将ResNet下采样带有r stride的特征图平铺为L × d大小的序列，发送到Transformer，其中L = H / r × W / r。<u>几个转置卷积和一个1×1卷积</u>被用来将Transformer输出上采样到<u>目标关键点热图大小K × H/4 × W/4</u>。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/transposeF3_20211211223244.png" style="zoom:80%;" />

图：参照的Transpose架构，非本文架构

**热图损失** 为了观察自我注意层自发学习的模式，我们首先只利用预测heatmap $\hat{H}_k$和ground-truth heatmap Hk之间的均方误差(MSE)损失来训练模型:
$$
L_{heatmap}=\frac{1}{K}\sum^K_{k=1}M\cdot \parallel \hat{H}_k -H_k \parallel
$$
其中<u>M是一个mask，遮罩了整个图像中的人群区域和小尺寸人群片段</u>。对模型进行热图损失训练后，关键点检测结果表明，训练后的模型能够准确定位多人关键点。

**naive self-attention** 我们从热图中获取关键点位置，并进一步可视化这些位置的关注区域。如图1的例子所示，使用朴素的自注意矩阵作为关联参考存在几个挑战:

 1)Transformer中有多个注意层，每个注意层显示出不同的特征。<u>选择哪些注意层作为关联参考</u>以及如何处理原始注意需要一个非常深思熟虑的融合和后处理策略。

2)虽然采样的大部分关键点位置都表现出局部的注意区域，特别是对其所属的人，但仍有一些关键点在距离较远的情况下自发地对他人的部分产生较高的注意得分。几乎不可能为所有情况确定一个完美的注意阈值，这使得关键点分组高度依赖于特定的实验观察。因此，<u>基于注意的分组不能保证关键点分配的正确性</u>，导致性能较差。

#### 2.3. 受实例掩膜监督的自我注意力

为了解决前面提到的使用naive自我注意分组关键点的挑战，我们提出了<u>监督自我注意</u>(**Supervise Self-Attention, SSA**)。理想情况下，<u>期望的注意模式应该是每个关键点位置只关注它所属的人实例</u>。人实例掩模中的值分布(0或1)提供了理想的引导信号，用于监督成对关键点的位置具有较低或较高的注意分数。然后，我们<u>提出了一种基于实例关键点位置的稀疏采样方法，对Transformer中自注意计算生成的具体注意矩阵进行监督</u>，如图2所示。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAf2_20211228151916.png" style="zoom:80%;" />

**图2. 模型概述**。模型体系结构由三个部分组成:一个常规**ResNet**、一个常规**Transformer编码器**和**几个转置的卷积层**。两种类型的损失函数被用来监督模型训练。模型的最终输出由groundtruth关键点热图监督。<u>直接的自我注意层之一**由实例掩码稀疏地监督**</u>。特别地，我们根据每个人实例的可见关键点位置对所选注意层的注意矩阵<u>行进行采样，将其重塑为类似于2d的maps</u>，然后使用每个实例的掩模来监督average map。在该图中，为了简单起见，我们只显示每个实例的几个关键点。

**Instance mask loss.** 我们假定第p-th个人的关节点GT位置是$\{(x^k_p,y^k_p,v^k_p)\}^K_{k=1}$, 其中 $v^k_p \in \{0,1\}$表示可见性，即为0表示无标注，为1 表示有标注。我们拿出Transformer中的某一中间层的注意力矩阵$A=Softmax(\frac{QK^T}{\sqrt{d}})\in \R^{L\times L}, 其中Q,K\in \R^{L\times d}$来进行监督。我们首先将注意力矩阵A调整为（h x w) x (h x w) 的维度，其中h=H/r, w=W/r. 然后将关键点坐标转换为下采样特征图的坐标系。<u>然后我们取出由这些位置指定的注意力矩阵相应的行</u>,其中一行大小为1x (h x w)，再将其reshape为h x w, 通道数为关键点数。因此，我们可以得到在每个关键点位置的重新塑造的注意图: $A[int(y^k_p/r),int(x^k_p/r),:,:]$ 。以一个人为例，我们根据其可见的关键点位置对注意力图进行采样和平均，以估计平均注意力图。我们称之为person attention map **Ap**:
$$
A_p=\frac{1}{\sum^K_{i=1}v^i_p}\sum^K_{k=1}v^k_p \cdot A[int(y^k_p/r),int(x^k_p/r),:,:]
$$
假设图像中第p个人的ground-truth实例掩码为$M_p\in \R^{\frac{H}{4}\times \frac{W}{4}}$，利用均方差损失函数对注意矩阵进行稀疏监督。由于softmax函数已将自我注意分数归一化，因此我们需要通过除以Ap的最大值来缩放Ap，以便缩放后的Ap更接近标注掩码的值范围(0或1)。注意到Ap的尺寸为$\frac{H}{r}\times \frac{W}{r}$而实例掩膜的GT值尺寸为$\frac{H}{4}\times \frac{W}{4}$，因此我们使用r/4次双线性插值来使Ap与实例掩膜尺寸相同。**实例掩码损失**为

$$
L_{mask}=MSE(bilinear(A_p/max(A_p)),M_p)=\frac{1}{N}\sum^N_{p=1}\parallel bilinear(A_p/max(A_p))-M_p\parallel
$$
**Objective.** 因此训练模型时的**总损失**为
$$
L_{train}=\alpha \cdot L_{heatmap}+ \beta \cdot L_{mask}
$$
其中α和β是平衡两种学习类型的两个系数。在Transformer的标准自注意计算中，注意矩阵是通过queries和keys的内积来计算的。它的梯度反向传播信息完全来自于后续的注意加权值之和。通过引入实例掩模损失来监督自注意，**监督注意矩阵**的**梯度学习**方向有两个**来源**: **关键点热图学习的隐式梯度信号** 和 **实例掩模学习的显式相似度约束**。选择α和β的近似值是训练模型的关键。我们设定α = 1， β = 0.01来平衡heatmap学习和mask学习。

#### 2.4. 关键点分组（Keypoints Grouping)

当训练良好的模型对给定的图像进行单次前向传递时，我们可以<u>从输出的关键点热图</u>和<u>即时注意层的监督注意矩阵</u>中解码出多人的人体姿势和掩模。我们**首先在一个7 x 7的局部窗口内对关键点热图进行非最大抑制**，<u>得到所有得分超过阈值t的局部最大位置</u>。**我们将所有这些候选点放入一个队列，并使用基于注意的算法将其解码为骨架**。<u>采用具有二次复杂度的自注意相似矩阵不可避免地会带来冗余计算</u>。然而，在某种程度上，这也对实例的关键点可能出现的位置和图像中的人数做出了最小的假设。接下来，我们从**身体优先**和**部分优先**两个角度提出了基于自我注意的算法。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAf3_20211228161633.png" style="zoom:80%;" />

**图3. 基于自我注意的分组。**当骨架中的已建立的关键点对一个未匹配的关键点产生更强的注意力吸引时，这个候选点将被分配到这个**骨架**中。蓝色边缘(厚的)比绿色边缘(薄的)有更高的注意力得分。

**2.4.1. Body-first view** 

这个视图旨在**从队列中逐个解码每个人的骨架**。假设我们在一个**队列**中通过**分数降序排列**所有类型的候选关键点，<u>我们弹出(pop)第一个关键点(可能是任何关键点类型)以建立一个新的骨架S，然后贪婪地从队列中找到最匹配的相邻候选关键点。</u>

对于具有**初始关键点的种子S**，**根据已定义的人体骨架运动学树，沿着搜索路径找到其他关键点**。在寻找某种类型的关节时，**骨架S**的已建立的关节(记为集合**Sf**)会诱导一个吸引盆地来“吸引”最有可能属于它的关节，如图3所示。对于关键点类型为k的候选集合Ck中的某一个**未匹配点pc** = (x, y, s)，我们使用当前发现的关键点与pc之间的**平均注意分数**作为**度量**，来衡量该骨架的吸引力:
$$
Attraction(p_c,S_f)=\frac{1}{S_f}\sum_{x',y',s'\in S_f }s' \cdot A[y,x,y',x'].
$$
因此，具有最高score×Attraction的候选点被认为属于当前骨架$S: p^∗_c = argmax_{p_c∈C_k}s·Attraction(p_c, S_f)$。我们重复上述过程并记录所有内容匹配的关键点直到找到骨架的所有关键点。

然后我们需要弹出下一个骨架。我们将第一个不匹配的关键点再次生成一个新的骨架S‘。我们按照前面的步骤找到属于这个实例的关键点。

注意，如果$Attraction(p_c, S'_f)$小于阈值λ(经验设置为0.0025)，那么这个骨架中的这类关键点就是空的(零填充)。同样值得注意的是，我们也考虑了一些关键点，这些关键点已经被先前的骨架S所收入，但只有当$Attraction(p_c, S'_f)>Attraction(p_c, S_f)$，我们将匹配的pc分配给当前骨架S’。

**2.4.4. Part-first view** 

这一观点旨在**对所有人类骨骼进行逐一解码**。给定每个关键点类型的所有候选对象，我们用最容易检测到的关键点(如鼻子)，初始化多个骨架种子{S1, S2，…, Sm}。然后我们按照固定的顺序将候选部件连接到当前的骨架。**这些骨架可以看作是由发现的关键点组成的多个集群**。与身体优先视图(body-first view)一样，我们也使用骨架中发现的<u>关键点的平均注意力吸引$Attraction(p_c, S^t_f)$作为分配候选部件的度量</u>(图3)。

但在部分优先视图(part-first view)中，我们计算候选部件和现有骨骼之间的成对距离矩阵，然后我们使用**匈牙利算法**(Kuhn, 1955)来解决这个**二部图匹配问题**。注意，如果在解决方案中表示匹配的$Attraction(p_c, S^t)$低于阈值λ，我们使用这个对应的候选部分开始一个新的骨架种子。我们重复上述过程，直到所有类型的候选部件都被分配。<u>这种局部优先分组算法虽然不能保证全局最优分配，但可以实现局部分配给骨架的最优解</u>。

我们<u>选择部分优先分组作为默认值</u>。并在附录A.5中对两种算法的性能、复杂度和运行时间进行了比较（如下表6所示，其中run time是整个流程的时间，包含了关键点检测、分组和实例分割，模型在GPU中实现，分组算法在CPU中实现）。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAt6_20211229132639.png" style="zoom:80%;" />

#### 2.5. 掩膜预测

当检测到的关键点被分组成骨架后，实例掩模很容易获得。为了产生实例分割结果，我们从第m个实例中取样可见的关键点位置$\{(\hat{x}^k_m, \hat{y}^k_m, \hat{v}^K_m)\}^K_{k=1}$，其监督自我注意矩阵为 $\hat{A}_m = \frac{\sum_k δ(\hat{v}^k_m > 0)·A[\hat{y}^k_m, \hat{x}^k_m，:，:]}{\sum_k δ(\hat{v}^k_m>0)} $。然后我们得到估计的实例掩码:$\hat{M}_m = \frac{A_m}{max(\hat{A}m)} > σ$，其中σ是一个阈值(默认为0.4)，用于确定掩码区域。当我们获得所有人实例的初始骨架和掩膜时，<u>一个人的关节可能落在多个不完整的骨骼中，但它们对应的部分(采样的注意区域)可能重叠</u>。因此，如果两个掩码的Intersection-over-Max (IoM)超过0.3，我们将进一步<u>执行非最大抑制来合并实例</u>，其中Max表示两个掩码之间的最大面积。

###  3. 实验结果

**数据集** 使用COCO数据集。

**模型设置**  我们按照**TransPose** 的模型架构设计来预测关键点热图。该设置建立在预先存在的**ResNet**和**Transformer Encoder**之上。我们使用Imagenet预先训练的ResNet-101或ResNet-151作为Backbone，其最后的分类层被一个1x1卷积代替，将通道从2048减少到d(192)。ResNet Backnone的正常输出步幅为32，但我们<u>通过增加扩张卷积和去除步幅来增加其最后阶段(C5阶段)的feature map分辨率</u>，即ResNet的下采样比r为16。我们使用具有6个编码器层的常规Transformer，每个层有一个关注头。FFN的隐藏维数为384。请参阅附录A.1中的更多训练和推理细节。

**效果**

我们主要比较了典型的自底向上的模型，因为我们也是Bottom-up的。主要比较了<u>OpenPose、PersonLab和AE</u>。跟随OpenPose和AE，我们同样使用单人姿态估计器优化了分组的骨架(refine)。我们采用**COCO预先训练的TransPose-R-A4**，它的结构与我们的模型非常相似，并且只有6M参数。我们将单姿态估计器应用于由包含人体掩模的box取得的每个单个scaled的人体区域。请注意，**refine结果高度依赖于分组和掩码预测的效果**，我们只在两个模型预测几乎相同的地方更新关键点估计。具体的更新规则是两个关键点之间的**关键点相似度**(keypoint similarity, KS)度量计算是否超过0.75，表明两个预测位置之间的距离已经很小。

从表1中可以看到，其实效果还是不如OpenPose、PersonLab这些bottom-up方法。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAt1_20211228213736.png" style="zoom:80%;" />

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAf4_20211228214146.png" style="zoom:80%;" />

我们进一步分析了**纯自底向上**的结果与通过基准测试和错误诊断工具**refine**的结果。我们将我们的方法与典型的OpenPose模型和基于Transformer的模型进行了比较。得出的定位误差条(图4)揭示了我们的模型的弱点和优点:

**(1)抖动误差(Jitter error)**: 多人存在下的热图定位精度仍然不如单人姿态估计的定位精度。纯自底向上模型的小的局部化和量化误差，降低了在较高阈值下的精度;

**(2) 关键点缺失误差(Missing error)**: 由于我们的算法并不能保证在检测到的位姿中，每个关键点的坐标都已被预测，如果对关键点的GT坐标进行标注，填零坐标会严重拉低计算出的OKS值。因此，为了进行评价，有必要做出完整的预测。<u>当我们进一步使用单人姿态估计器来填充初始分组骨架中得分为0的缺失关节时，获得了约7 AP增益(表1)</u>，并减少了缺失误差(如图4(d)所示);   （ ？？？那下一步是不是需要考虑优化算法了，这样看来漏掉的关键点有点多呀

**(3)反演误差(Inversion error): **迫使单个实例中不同的关键点类型具有更高的查询键相似度，可能会使模型难以区分不同的关键点类型，特别是左右相反;

**(4)交换错误(Swap error):** 我们注意到我们的纯自底向上模型有更少的交换错误(1.2%，如图4(c)所示)，这表示不同实例中语义相似部分之间的混淆更少。 （大概是用了实例掩码作为监督的好处吧）

结果表明，与OpenPose模型相比，基于注意力的分组策略能够更好地将零件分配给相应的实例。我们在附录A.6中展示了定性的人体姿势和实例分割结果。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAt2_20211228214929231.png" alt="image-20211228214929231" style="zoom:80%;" />

**人体实例分割** 我们评估实例分割的结果在COCO val split(仅人类别)。我们将我们的方法与**PersonLab**进行了比较。在表3中，由于COCO人员关键点评估协议的约定，我们报告了最多20人的结果。在平均精度(AP)上的结果表明，<u>与PersonLab相比，我们的模型在分割性能上仍有差距</u>。我们认为这主要是因为<u>我们在低分辨率的注意图上进行了掩模学习</u>，这些注意图已经在640x640或800x800输入分辨率下采样了16次，而报告的PersonLab结果是基于8次下采样w.r.t的1401x1401输入分辨率。如表3所示，我们的模型在小型和中型规模上表现较差，但在大规模人员上也能实现和PersonLab相当甚至更好的性能，即使PersonLab使用更大的分辨率。在本文中，实例分割并不是我们的主要目标，因此我们直接使用16次双线性插值对注意图上采样作为最终的分割结果。我们相信进一步的特定于掩码的优化可以提高实例分割的性能。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAt3_20211228215001.png" style="zoom:80%;" />

#### 3.2. 监督自注意力和朴素自注意力的比较

为了研究在有和没有监督自我注意的情况下训练模型学习的差异，我们比较了它们在热图损失和实例掩模损失方面的收敛性，因为COCO训练数据上的过拟合通常不是问题。如图5所示，与训练朴素的自我注意模型相比，<u>监督自我注意在mask学习中获得了更好的拟合效果，同时对heatmap学习的拟合做出了可以接受的牺牲</u>。值得注意的是，朴素自我注意模型的实例掩模训练损失曲线略有下降，<u>说明自发形成的注意模式具有实例意识的倾向</u>。为了定量评估使用<u>朴素自注意模式进行关键点分组</u>的性能，我们<u>将所有Transformer层的注意平均作为关联参考</u>(如图1所示)。在基于(res152, s16, i640)的监督自我注意模型的测试设置和分组算法中，我们在COCO验证集上实现了29.0AP，远低于监督自我注意的50.7AP。

<img src="https://gitee.com/EileenWang/Figurebed/raw/main/imgs/ATWYAf5_20211229142739.png" style="zoom:80%;" />

### 4. 相关工作

**Transformer** 略。

**姿态识别和实例分割**

多人姿态估计方法通常分为两类:自顶向下(Top-down)和自底向上(Bottom-up)。**Top-down模型**首先检测人，然后估计每个人的姿态，如G-RMI 、Mask-RCNN 、CPN 、SimpleBaseline和HRNet。**Bottom-up模型**需要<u>在任意位置和尺度上检测各种类型关键点的存在</u>。将关键点与实例匹配需要模型学习人类知识预先定义的密集关联信号。**OpenPose** 提出部分亲和度场(part affinity field, PAF)，通过计算连接线上的积分来度量关键点之间的关联。关联嵌入(**Associative Embedding**)将嵌入抽象为人类的“标签”ID，以衡量关联。**PersonLab**<u>构建中距偏移作为几何嵌入</u>，将关键点分组到实例中。此外，单阶段方法(Zhou et al.， 2019;Nie等人，2019)也<u>回归偏移场</u>，将关键点分配到他们的中心。与它们相比，我们<u>使用Transformer来捕获一个人内部的依赖项和不同人之间的相互依赖项</u>。我们明确地利用自注意机制的内在特性来解决关联问题，而不是回归高度抽象的偏移域或嵌入。

通用的**实例分割**方法也可以分为**自顶向下**和**自底向上**两种模式。**自顶向下**方法基于<u>object proposals 预测实例掩模</u>，如**FCIS** 和**Mask-RCNN**。**自底向上**的方法主要是<u>将语义分割结果聚类，使用嵌入空间或区分损失来度量像素关联</u>，以获得实例分割。与它们相比，我们的方法使用<u>自注意来衡量关联，并根据实例关键点估计实例掩码</u>。

### 5. 讨论和未来展望

提出了一种**利用Transformer解决关键点检测和实例关联问题**的新方法。通过**监督自注意的内在特征**和<u>任意对位置之间的特征相似性</u>来<u>解决关键点或像素的分组问题</u>。与典型的基于CNN的自底向上模型不同，<u>它不再需要预定义的向量场或嵌入作为关联参考</u>，从而减少了模型冗余，简化了管道。我们在具有挑战性的COCO关键点检测和人实例分割任务上证明了该方法的有效性和简洁性。

目前的方法也带来了局限性和挑战。由于标准Transformer的**二次复杂度**，该模型仍然在同时扩大Transformer容量和输入图像的分辨率方面挣扎。**损失标准、模型体系结构和训练过程的选择可以进一步优化**。此外，对实例掩码注释的依赖也可以在以后的工作中消除，比如只对关键点位置施加高关注度和低关注度约束。<u>尽管目前的方法还没有打败基于CNN的先进的同类方法</u>，但我们相信，它有望利用或监督视觉tansformer中的自我注意，以解决多人姿态估计和其他任务或应用中的检测和关联问题。
